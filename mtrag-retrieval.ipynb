{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smen/.conda/envs/mtrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# def load_bge(device=None, model_name=\"bge15small\"):\n",
    "#     if device is None:\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(\"Using device:\", device)\n",
    "\n",
    "#     # 官方 baseline 建议：推理用 FP32，别 .half()，保证排序稳定\n",
    "#     model = SentenceTransformer(model_name, device=device, local_files_only=True)\n",
    "#     model = model.half()\n",
    "#     model.max_seq_length = 512\n",
    "\n",
    "#     def encode(texts, batch_size=256):\n",
    "#         all_emb = []\n",
    "#         for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "#             batch = texts[i:i+batch_size]\n",
    "#             emb = model.encode(\n",
    "#                 batch,\n",
    "#                 convert_to_tensor=True,\n",
    "#                 batch_size=batch_size,\n",
    "#                 device=device,\n",
    "#                 show_progress_bar=False,\n",
    "#                 normalize_embeddings=True,\n",
    "#             )\n",
    "#             all_emb.append(emb)\n",
    "#         return torch.cat(all_emb, dim=0)\n",
    "\n",
    "#     # 给自己挂一个 encode_cuda 方便后面调用\n",
    "#     model.encode_cuda = encode\n",
    "#     return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smen/.conda/envs/mtrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.57.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_kingsoft(model_name=\"Kingsoft-LLM/QZhou-Embedding\", device=\"cuda\"):\n",
    "#     \"\"\"\n",
    "#     Kingsoft-QZhou embedding loader with batching and encode_cuda().\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(\"Using device:\", device)\n",
    "\n",
    "#     # Important: Kingsoft uses trust_remote_code + custom forward\n",
    "#     model = SentenceTransformer(\n",
    "#         model_name,\n",
    "#         device=device,\n",
    "#         trust_remote_code=True,\n",
    "#         model_kwargs={\n",
    "#             \"device_map\": device,\n",
    "#             \"trust_remote_code\": True\n",
    "#         },\n",
    "#         tokenizer_kwargs={\n",
    "#             \"padding_side\": \"left\",\n",
    "#             \"trust_remote_code\": True\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # FP16 is OK for Kingsoft (not like BGE baseline)\n",
    "#     model = model.half()\n",
    "\n",
    "#     # Kingsoft max length = 2048, but 512 is OK for BEIR/MTRAG\n",
    "#     model.max_seq_length = 512\n",
    "\n",
    "#     # -----------------------------\n",
    "#     #        Batch Encoder\n",
    "#     # -----------------------------\n",
    "#     def encode(texts, batch_size=64, prompt_name=None):\n",
    "#         \"\"\"\n",
    "#         texts: list[str]\n",
    "#         prompt_name: \"query\" or \"document\"\n",
    "#         \"\"\"\n",
    "#         all_emb = []\n",
    "\n",
    "#         for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "#             batch = texts[i:i + batch_size]\n",
    "\n",
    "#             emb = model.encode(\n",
    "#                 batch,\n",
    "#                 convert_to_tensor=True,\n",
    "#                 device=device,\n",
    "#                 show_progress_bar=False,\n",
    "#                 batch_size=batch_size,\n",
    "#                 normalize_embeddings=True,\n",
    "#                 prompt_name=prompt_name,   # KINGSOFT FEATURE\n",
    "#             )\n",
    "\n",
    "#             all_emb.append(emb)\n",
    "\n",
    "#         return torch.cat(all_emb, dim=0)\n",
    "\n",
    "#     # Attach helper\n",
    "#     model.encode_cuda = encode\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_qwen3_st(model_name=\"Qwen3-Embedding-4B\", device=None):\n",
    "    \"\"\"\n",
    "    Load Qwen3 embedding model using SentenceTransformers.\n",
    "\n",
    "    Notes:\n",
    "      - flash_attention_2 is DISABLED (your GPU cannot use it)\n",
    "      - SentenceTransformers will:\n",
    "            • handle query/document prompts\n",
    "            • handle left padding\n",
    "            • handle pooling + normalization\n",
    "      - We only provide efficient batching through encode_cuda()\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # ---- SAFE FOR HPC ----\n",
    "    model = SentenceTransformer(\n",
    "        model_name,\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"cuda\",      # works on HPC\n",
    "            \"trust_remote_code\": True,\n",
    "            \"attn_implementation\": None,   # <- IMPORTANT: disable flash-attention2\n",
    "        },\n",
    "        tokenizer_kwargs={\n",
    "            \"padding_side\": \"left\",    # Recommended by Qwen team\n",
    "            \"trust_remote_code\": True,\n",
    "        },\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "        \n",
    "    )\n",
    "    model.max_seq_length = 512\n",
    "\n",
    "    # ------------------------------\n",
    "    # Custom batch encode wrapper\n",
    "    # ------------------------------\n",
    "    def encode(texts, batch_size=32, prompt_name=None):\n",
    "        \"\"\"\n",
    "        Batch embed texts using model.encode() internally.\n",
    "        prompt_name:\n",
    "            - None        → document encoding\n",
    "            - \"query\"     → query prompt from Qwen config\n",
    "        \"\"\"\n",
    "        all_emb = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "\n",
    "            emb = model.encode(\n",
    "                batch,\n",
    "                batch_size=batch_size,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True,     # IMPORTANT for retrieval\n",
    "                prompt_name=prompt_name,\n",
    "            )\n",
    "\n",
    "            all_emb.append(emb.cpu())\n",
    "\n",
    "        return torch.cat(all_emb, dim=0)\n",
    "\n",
    "    # attach method\n",
    "    model.encode_cuda = encode\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_corpus(corpus_path):\n",
    "    corpus = {}\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            doc_id = item[\"document_id\"] if \"document_id\" in item else item[\"_id\"]\n",
    "            title = item.get(\"title\", \"\")\n",
    "            text = item.get(\"text\", \"\")\n",
    "            corpus[doc_id] = {\n",
    "                \"title\": title,\n",
    "                \"text\": text,\n",
    "            }\n",
    "    return corpus\n",
    "\n",
    "def load_queries(query_path):\n",
    "    queries = {}\n",
    "    with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            qid = item.get(\"_id\") or item.get(\"query_id\") or item[\"task_id\"]\n",
    "            text = item[\"text\"]\n",
    "            queries[qid] = text\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def run_retrieval_for_collection(name, cfg, model, top_k=50):\n",
    "#     root = cfg[\"root\"]\n",
    "#     corpus_path = os.path.join(root, cfg[\"corpus_file\"])\n",
    "#     query_path = os.path.join(root, cfg[\"query_file\"])\n",
    "\n",
    "#     print(f\"\\n========== Collection: {name} ==========\")\n",
    "#     print(\"Corpus:\", corpus_path)\n",
    "#     print(\"Queries:\", query_path)\n",
    "\n",
    "#     corpus = load_corpus(corpus_path)\n",
    "#     queries = load_queries(query_path)\n",
    "\n",
    "#     # 1) encode corpus\n",
    "#     doc_ids = list(corpus.keys())\n",
    "#     doc_texts = [(corpus[d].get(\"title\", \"\") + \" \" + corpus[d].get(\"text\", \"\")).strip()\n",
    "#                  for d in doc_ids]\n",
    "#     # doc_emb = model.encode_cuda(doc_texts, batch_size=8)\n",
    "#     doc_emb = model.encode_cuda(\n",
    "#         doc_texts,\n",
    "#         batch_size=64,\n",
    "#         prompt_name=\"document\"\n",
    "#     )\n",
    "\n",
    "#     print(\"doc_emb shape:\", doc_emb.shape)\n",
    "\n",
    "#     # 2) encode queries\n",
    "#     q_ids = list(queries.keys())\n",
    "#     q_texts = [queries[q] for q in q_ids]\n",
    "#     # q_emb = model.encode_cuda(q_texts, batch_size=8)\n",
    "#     q_emb = model.encode_cuda(\n",
    "#         q_texts,\n",
    "#         batch_size=64,\n",
    "#         prompt_name=\"query\"\n",
    "#     )\n",
    "\n",
    "#     print(\"q_emb shape:\", q_emb.shape)\n",
    "\n",
    "#     # 3) brute-force retrieval (cosine = dot，因为已经 normalize)\n",
    "#     sims = torch.matmul(q_emb, doc_emb.T)   # [num_q, num_doc]\n",
    "\n",
    "#     results = []  # 用于写 JSONL 的行\n",
    "#     for qi, qid in enumerate(tqdm(q_ids, desc=\"Building results\")):\n",
    "#         scores = sims[qi]\n",
    "#         vals, idx = torch.topk(scores, top_k)\n",
    "#         ctxs = []\n",
    "#         for v, ix in zip(vals.tolist(), idx.tolist()):\n",
    "#             ctxs.append({\n",
    "#                 \"document_id\": doc_ids[ix],\n",
    "#                 \"score\": float(v),\n",
    "#             })\n",
    "\n",
    "#         results.append({\n",
    "#             \"task_id\": qid,                  # 对应 run_retrieval_eval 里的 query_id\n",
    "#             \"contexts\": ctxs,\n",
    "#             \"Collection\": cfg[\"collection_name\"],\n",
    "#         })\n",
    "\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval_for_collection(name, cfg, model, top_k=50):\n",
    "    import os, torch\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    root = cfg[\"root\"]\n",
    "    corpus_path = os.path.join(root, cfg[\"corpus_file\"])\n",
    "    query_path = os.path.join(root, cfg[\"query_file\"])\n",
    "\n",
    "    print(f\"\\n========== Collection: {name} ==========\")\n",
    "    print(\"Corpus:\", corpus_path)\n",
    "    print(\"Queries:\", query_path)\n",
    "\n",
    "    # Load data\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    queries = load_queries(query_path)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Encode documents (NO PROMPT)\n",
    "    # ------------------------------\n",
    "    doc_ids = list(corpus.keys())\n",
    "    doc_texts = [\n",
    "        (corpus[d].get(\"title\", \"\") + \" \" + corpus[d].get(\"text\", \"\")).strip()\n",
    "        for d in doc_ids\n",
    "    ]\n",
    "\n",
    "    doc_emb = model.encode_cuda(doc_texts, batch_size=128, prompt_name=None)\n",
    "    print(\"doc_emb shape:\", doc_emb.shape)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Encode queries (WITH Qwen3 query prompt)\n",
    "    # ------------------------------\n",
    "    q_ids = list(queries.keys())\n",
    "    q_texts = [queries[q] for q in q_ids]\n",
    "\n",
    "    q_emb = model.encode_cuda(q_texts, batch_size=128, prompt_name=\"query\")\n",
    "    print(\"q_emb shape:\", q_emb.shape)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cosine similarity (already normalized)\n",
    "    # ------------------------------\n",
    "    sims = torch.matmul(q_emb, doc_emb.T)\n",
    "\n",
    "    results = []\n",
    "    for qi, qid in enumerate(tqdm(q_ids, desc=\"Building results\")):\n",
    "        vals, idx = torch.topk(sims[qi], top_k)\n",
    "        ctxs = [\n",
    "            {\"document_id\": doc_ids[j], \"score\": float(v)}\n",
    "            for v, j in zip(vals.tolist(), idx.tolist())\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            \"task_id\": qid,\n",
    "            \"contexts\": ctxs,\n",
    "            \"Collection\": cfg[\"collection_name\"],\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_submission(submission_path):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model = load_bge(model_name=model_name, device=device)\n",
    "    # model = load_qwen(model_name=\"Qwen3\")\n",
    "    # model = load_kingsoft(model_name=model_name, device=device)\n",
    "    model = load_qwen3_st(model_name=\"Qwen3-Embedding-0.6B\", device=device)\n",
    "\n",
    "    all_results = []\n",
    "    for name, cfg in COLLECTIONS.items():\n",
    "        res = run_retrieval_for_collection(name, cfg, model, top_k=50)\n",
    "        all_results.extend(res)\n",
    "\n",
    "    print(f\"\\nTotal tasks: {len(all_results)}\")\n",
    "    with open(submission_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in all_results:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Saved submission to:\", submission_path)\n",
    "    return submission_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_official_eval(input_file, output_file):\n",
    "    cmd = [\n",
    "        \"python3\",\n",
    "        \"scripts/evaluation/run_retrieval_eval.py\",\n",
    "        \"--input_file\", input_file,\n",
    "        \"--output_file\", output_file,\n",
    "        \"--model_name\", model_name,\n",
    "        \"--task_name\", task_name\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(\"Done, scored file:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# del model\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": \"clapnq_rewrite.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": \"fiqa_rewrite.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": \"govt_rewrite.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": \"cloud_rewrite.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"Qwen3\"\n",
    "task_name = \"rewrite\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python3 scripts/evaluation/run_retrieval_eval.py --input_file outputs/Qwen3_rewrite.jsonl --output_file outputs/Qwen3_rewrite_score.jsonl --model_name Qwen3 --task_name rewrite\n",
      "\n",
      "collection_name: mt-rag-fiqa-beir-elser-512-100-20240501\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.37778, 0.32857, 0.34654, 0.39561], 'Recall': [0.15677, 0.30636, 0.3646, 0.47837], 'collection': 'mt-rag-fiqa-beir-elser-512-100-20240501', 'count': 180}\n",
      "Weighted average Recall: [0.15677, 0.30636, 0.3646, 0.47837]\n",
      "Weighted average nDCG: [0.37778, 0.32857, 0.34654, 0.39561]\n",
      "Done, scored file: outputs/Qwen3_rewrite_score.jsonl\n"
     ]
    }
   ],
   "source": [
    "run_official_eval(SUB_PATH, SCORED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Collection: clapnq ==========\n",
      "Corpus: human/retrieval_tasks/clapnq/clapnq.jsonl\n",
      "Queries: human/retrieval_tasks/clapnq/clapnq_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding:   1%|          | 11/1433 [01:00<2:09:40,  5.47s/it]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x2ac9e0ee1990>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/smen/.conda/envs/mtrag/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m SUB_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m SCORED_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_score.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m sub_path \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSUB_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m run_official_eval(SUB_PATH, SCORED_PATH)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mbuild_submission\u001b[0;34m(submission_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, cfg \u001b[38;5;129;01min\u001b[39;00m COLLECTIONS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 10\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_retrieval_for_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(res)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mrun_retrieval_for_collection\u001b[0;34m(name, cfg, model, top_k)\u001b[0m\n\u001b[1;32m     20\u001b[0m doc_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(corpus\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     21\u001b[0m doc_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     (corpus[d]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m corpus[d]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m doc_ids\n\u001b[1;32m     24\u001b[0m ]\n\u001b[0;32m---> 26\u001b[0m doc_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_emb shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, doc_emb\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Encode queries (WITH Qwen3 query prompt)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m, in \u001b[0;36mload_qwen3_st.<locals>.encode\u001b[0;34m(texts, batch_size, prompt_name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     54\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 56\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# IMPORTANT for retrieval\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     all_emb\u001b[38;5;241m.\u001b[39mappend(emb\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(all_emb, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1094\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1094\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1096\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1175\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m   1170\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1171\u001b[0m             key: value\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1173\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[1;32m   1174\u001b[0m         }\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:261\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward_params}\n\u001b[0;32m--> 261\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 410\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    423\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    424\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    425\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:258\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    257\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 258\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    261\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    262\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    269\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/mtrag/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:62\u001b[0m, in \u001b[0;36mQwen3RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     60\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     61\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 62\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_name = \"lastturn\"\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": f\"clapnq_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": f\"fiqa_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": f\"govt_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": f\"cloud_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"Qwen3\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Collection: clapnq ==========\n",
      "Corpus: human/retrieval_tasks/clapnq/clapnq.jsonl\n",
      "Queries: human/retrieval_tasks/clapnq/clapnq_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 717/717 [02:21<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([183408, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([208, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 208/208 [00:00<00:00, 5451.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: fiqa ==========\n",
      "Corpus: human/retrieval_tasks/fiqa/fiqa.jsonl\n",
      "Queries: human/retrieval_tasks/fiqa/fiqa_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 239/239 [00:48<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([61022, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([180, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 180/180 [00:00<00:00, 7061.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: govt ==========\n",
      "Corpus: human/retrieval_tasks/govt/govt.jsonl\n",
      "Queries: human/retrieval_tasks/govt/govt_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 194/194 [00:43<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([49607, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([201, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 201/201 [00:00<00:00, 6743.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: cloud ==========\n",
      "Corpus: human/retrieval_tasks/cloud/cloud.jsonl\n",
      "Queries: human/retrieval_tasks/cloud/cloud_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 283/283 [01:00<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([72442, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([188, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 188/188 [00:00<00:00, 3644.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tasks: 777\n",
      "Saved submission to: outputs/bge15small_lastturn.jsonl\n",
      "Running: python3 scripts/evaluation/run_retrieval_eval.py --input_file outputs/bge15small_lastturn.jsonl --output_file outputs/bge15small_lastturn_score.jsonl --model_name bge15small --task_name lastturn\n",
      "\n",
      "collection_name: mt-rag-clapnq-elser-512-100-20240503\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.37019, 0.34252, 0.36196, 0.41205], 'Recall': [0.15309, 0.31756, 0.38527, 0.50286], 'collection': 'mt-rag-clapnq-elser-512-100-20240503', 'count': 208}\n",
      "\n",
      "collection_name: mt-rag-fiqa-beir-elser-512-100-20240501\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.21667, 0.1931, 0.20177, 0.23167], 'Recall': [0.09265, 0.17552, 0.20681, 0.27622], 'collection': 'mt-rag-fiqa-beir-elser-512-100-20240501', 'count': 180}\n",
      "\n",
      "collection_name: mt-rag-govt-elser-512-100-20240611\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.22388, 0.2174, 0.2467, 0.28268], 'Recall': [0.10661, 0.20652, 0.27866, 0.36123], 'collection': 'mt-rag-govt-elser-512-100-20240611', 'count': 201}\n",
      "\n",
      "collection_name: mt-rag-ibmcloud-elser-512-100-20240502\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.23404, 0.21409, 0.23745, 0.27052], 'Recall': [0.11401, 0.20599, 0.26207, 0.34106], 'collection': 'mt-rag-ibmcloud-elser-512-100-20240502', 'count': 188}\n",
      "Weighted average Recall: [0.11760902187902188, 0.22893528957528958, 0.2865402574002574, 0.3745701287001287]\n",
      "Weighted average nDCG: [0.2638346460746461, 0.24446393822393822, 0.26490808236808233, 0.30255268983268985]\n",
      "Done, scored file: outputs/bge15small_lastturn_score.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_name = \"lastturn\"\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": f\"clapnq_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": f\"fiqa_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": f\"govt_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": f\"cloud_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"bge15small\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Collection: clapnq ==========\n",
      "Corpus: human/retrieval_tasks/clapnq/clapnq.jsonl\n",
      "Queries: human/retrieval_tasks/clapnq/clapnq_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 717/717 [02:21<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([183408, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([208, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 208/208 [00:00<00:00, 508.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: fiqa ==========\n",
      "Corpus: human/retrieval_tasks/fiqa/fiqa.jsonl\n",
      "Queries: human/retrieval_tasks/fiqa/fiqa_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 239/239 [00:48<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([61022, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 14.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([180, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 180/180 [00:00<00:00, 2538.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: govt ==========\n",
      "Corpus: human/retrieval_tasks/govt/govt.jsonl\n",
      "Queries: human/retrieval_tasks/govt/govt_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 194/194 [00:42<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([49607, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([201, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 201/201 [00:00<00:00, 2868.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: cloud ==========\n",
      "Corpus: human/retrieval_tasks/cloud/cloud.jsonl\n",
      "Queries: human/retrieval_tasks/cloud/cloud_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 283/283 [01:00<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([72442, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([188, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 188/188 [00:00<00:00, 2841.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tasks: 777\n",
      "Saved submission to: outputs/bge15small_questions.jsonl\n",
      "Running: python3 scripts/evaluation/run_retrieval_eval.py --input_file outputs/bge15small_questions.jsonl --output_file outputs/bge15small_questions_score.jsonl --model_name bge15small --task_name questions\n",
      "\n",
      "collection_name: mt-rag-govt-elser-512-100-20240611\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.15423, 0.1614, 0.18184, 0.21165], 'Recall': [0.07222, 0.16119, 0.20978, 0.28226], 'collection': 'mt-rag-govt-elser-512-100-20240611', 'count': 201}\n",
      "\n",
      "collection_name: mt-rag-fiqa-beir-elser-512-100-20240501\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.1, 0.09567, 0.10404, 0.12082], 'Recall': [0.03981, 0.09491, 0.11435, 0.15296], 'collection': 'mt-rag-fiqa-beir-elser-512-100-20240501', 'count': 180}\n",
      "\n",
      "collection_name: mt-rag-ibmcloud-elser-512-100-20240502\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.12234, 0.12076, 0.14182, 0.17836], 'Recall': [0.0516, 0.12181, 0.16968, 0.25833], 'collection': 'mt-rag-ibmcloud-elser-512-100-20240502', 'count': 188}\n",
      "\n",
      "collection_name: mt-rag-clapnq-elser-512-100-20240503\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.25, 0.21368, 0.2333, 0.27778], 'Recall': [0.09928, 0.1895, 0.24535, 0.34626], 'collection': 'mt-rag-clapnq-elser-512-100-20240503', 'count': 208}\n",
      "Weighted average Recall: [0.06696661518661519, 0.14388580437580437, 0.18749217503217502, 0.26364888030888034]\n",
      "Weighted average nDCG: [0.15958835263835264, 0.1503350321750322, 0.1679093951093951, 0.20025633204633206]\n",
      "Done, scored file: outputs/bge15small_questions_score.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_name = \"questions\"\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": f\"clapnq_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": f\"fiqa_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": f\"govt_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": f\"cloud_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"bge15small\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Collection: clapnq ==========\n",
      "Corpus: human/retrieval_tasks/clapnq/clapnq.jsonl\n",
      "Queries: human/retrieval_tasks/clapnq/clapnq_rewrite.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 717/717 [05:21<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([183408, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 31.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([208, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 208/208 [00:00<00:00, 271.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: fiqa ==========\n",
      "Corpus: human/retrieval_tasks/fiqa/fiqa.jsonl\n",
      "Queries: human/retrieval_tasks/fiqa/fiqa_rewrite.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 239/239 [01:48<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([61022, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([180, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 180/180 [00:00<00:00, 5023.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: govt ==========\n",
      "Corpus: human/retrieval_tasks/govt/govt.jsonl\n",
      "Queries: human/retrieval_tasks/govt/govt_rewrite.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 194/194 [01:28<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([49607, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([201, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 201/201 [00:00<00:00, 4717.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: cloud ==========\n",
      "Corpus: human/retrieval_tasks/cloud/cloud.jsonl\n",
      "Queries: human/retrieval_tasks/cloud/cloud_rewrite.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 283/283 [02:09<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([72442, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([188, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 188/188 [00:00<00:00, 3633.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tasks: 777\n",
      "Saved submission to: outputs/bge15_rewrite.jsonl\n",
      "Running: python3 scripts/evaluation/run_retrieval_eval.py --input_file outputs/bge15_rewrite.jsonl --output_file outputs/bge15_rewrite_score.jsonl --model_name bge15 --task_name rewrite\n",
      "\n",
      "collection_name: mt-rag-ibmcloud-elser-512-100-20240502\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.23936, 0.24299, 0.27315, 0.30837], 'Recall': [0.12668, 0.24628, 0.31055, 0.38999], 'collection': 'mt-rag-ibmcloud-elser-512-100-20240502', 'count': 188}\n",
      "\n",
      "collection_name: mt-rag-clapnq-elser-512-100-20240503\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.42308, 0.36574, 0.40129, 0.45911], 'Recall': [0.15585, 0.32841, 0.43843, 0.57165], 'collection': 'mt-rag-clapnq-elser-512-100-20240503', 'count': 208}\n",
      "\n",
      "collection_name: mt-rag-govt-elser-512-100-20240611\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.28856, 0.27288, 0.3034, 0.35744], 'Recall': [0.12927, 0.26672, 0.33701, 0.46512], 'collection': 'mt-rag-govt-elser-512-100-20240611', 'count': 201}\n",
      "\n",
      "collection_name: mt-rag-fiqa-beir-elser-512-100-20240501\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.25, 0.21587, 0.24277, 0.27617], 'Recall': [0.10255, 0.1921, 0.2621, 0.34146], 'collection': 'mt-rag-fiqa-beir-elser-512-100-20240501', 'count': 180}\n",
      "Weighted average Recall: [0.12956873873873873, 0.2610021106821107, 0.3404039253539254, 0.44681240669240674]\n",
      "Weighted average nDCG: [0.3037334362934363, 0.27729925353925355, 0.30824005148005146, 0.35395685971685975]\n",
      "Done, scored file: outputs/bge15_rewrite_score.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_name = \"rewrite\"\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": f\"clapnq_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": f\"fiqa_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": f\"govt_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": f\"cloud_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"bge15\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Collection: clapnq ==========\n",
      "Corpus: human/retrieval_tasks/clapnq/clapnq.jsonl\n",
      "Queries: human/retrieval_tasks/clapnq/clapnq_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 717/717 [05:15<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([183408, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([208, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 208/208 [00:00<00:00, 3679.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: fiqa ==========\n",
      "Corpus: human/retrieval_tasks/fiqa/fiqa.jsonl\n",
      "Queries: human/retrieval_tasks/fiqa/fiqa_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 239/239 [01:48<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([61022, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([180, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 180/180 [00:00<00:00, 4021.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: govt ==========\n",
      "Corpus: human/retrieval_tasks/govt/govt.jsonl\n",
      "Queries: human/retrieval_tasks/govt/govt_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 194/194 [01:28<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([49607, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([201, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 201/201 [00:00<00:00, 3472.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: cloud ==========\n",
      "Corpus: human/retrieval_tasks/cloud/cloud.jsonl\n",
      "Queries: human/retrieval_tasks/cloud/cloud_lastturn.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 283/283 [02:09<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([72442, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([188, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 188/188 [00:00<00:00, 3787.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tasks: 777\n",
      "Saved submission to: outputs/bge15_lastturn.jsonl\n",
      "Running: python3 scripts/evaluation/run_retrieval_eval.py --input_file outputs/bge15_lastturn.jsonl --output_file outputs/bge15_lastturn_score.jsonl --model_name bge15 --task_name lastturn\n",
      "\n",
      "collection_name: mt-rag-ibmcloud-elser-512-100-20240502\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.21277, 0.20314, 0.22704, 0.25412], 'Recall': [0.10496, 0.20009, 0.25603, 0.31948], 'collection': 'mt-rag-ibmcloud-elser-512-100-20240502', 'count': 188}\n",
      "\n",
      "collection_name: mt-rag-clapnq-elser-512-100-20240503\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.33654, 0.29499, 0.32687, 0.38005], 'Recall': [0.12115, 0.26647, 0.35971, 0.48631], 'collection': 'mt-rag-clapnq-elser-512-100-20240503', 'count': 208}\n",
      "\n",
      "collection_name: mt-rag-govt-elser-512-100-20240611\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.19403, 0.20913, 0.23521, 0.27631], 'Recall': [0.08765, 0.20954, 0.27056, 0.36388], 'collection': 'mt-rag-govt-elser-512-100-20240611', 'count': 201}\n",
      "\n",
      "collection_name: mt-rag-fiqa-beir-elser-512-100-20240501\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.19444, 0.18049, 0.19378, 0.22807], 'Recall': [0.08185, 0.16487, 0.20575, 0.28862], 'collection': 'mt-rag-fiqa-beir-elser-512-100-20240501', 'count': 180}\n",
      "Weighted average Recall: [0.09946245817245818, 0.21214519948519947, 0.27589559845559847, 0.36847644787644784]\n",
      "Weighted average nDCG: [0.23680863577863578, 0.22403033462033464, 0.24817257400257398, 0.2875365122265122]\n",
      "Done, scored file: outputs/bge15_lastturn_score.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_name = \"lastturn\"\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": f\"clapnq_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": f\"fiqa_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": f\"govt_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": f\"cloud_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"bge15\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Collection: clapnq ==========\n",
      "Corpus: human/retrieval_tasks/clapnq/clapnq.jsonl\n",
      "Queries: human/retrieval_tasks/clapnq/clapnq_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 717/717 [05:15<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([183408, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([208, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 208/208 [00:00<00:00, 1311.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: fiqa ==========\n",
      "Corpus: human/retrieval_tasks/fiqa/fiqa.jsonl\n",
      "Queries: human/retrieval_tasks/fiqa/fiqa_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 239/239 [01:48<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([61022, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([180, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 180/180 [00:00<00:00, 1574.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: govt ==========\n",
      "Corpus: human/retrieval_tasks/govt/govt.jsonl\n",
      "Queries: human/retrieval_tasks/govt/govt_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 194/194 [01:28<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([49607, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([201, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 201/201 [00:00<00:00, 1233.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Collection: cloud ==========\n",
      "Corpus: human/retrieval_tasks/cloud/cloud.jsonl\n",
      "Queries: human/retrieval_tasks/cloud/cloud_questions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 283/283 [02:08<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_emb shape: torch.Size([72442, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb shape: torch.Size([188, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building results: 100%|██████████| 188/188 [00:00<00:00, 1087.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tasks: 777\n",
      "Saved submission to: outputs/bge15_questions.jsonl\n",
      "Running: python3 scripts/evaluation/run_retrieval_eval.py --input_file outputs/bge15_questions.jsonl --output_file outputs/bge15_questions_score.jsonl --model_name bge15 --task_name questions\n",
      "\n",
      "collection_name: mt-rag-ibmcloud-elser-512-100-20240502\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.09043, 0.12217, 0.14433, 0.16774], 'Recall': [0.03785, 0.13378, 0.18227, 0.23821], 'collection': 'mt-rag-ibmcloud-elser-512-100-20240502', 'count': 188}\n",
      "\n",
      "collection_name: mt-rag-govt-elser-512-100-20240611\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.14428, 0.14106, 0.16065, 0.18921], 'Recall': [0.06667, 0.14154, 0.18441, 0.25307], 'collection': 'mt-rag-govt-elser-512-100-20240611', 'count': 201}\n",
      "\n",
      "collection_name: mt-rag-fiqa-beir-elser-512-100-20240501\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.1, 0.09135, 0.10274, 0.11544], 'Recall': [0.04769, 0.08889, 0.11481, 0.14602], 'collection': 'mt-rag-fiqa-beir-elser-512-100-20240501', 'count': 180}\n",
      "\n",
      "collection_name: mt-rag-clapnq-elser-512-100-20240503\n",
      "Retriever Evaluation Aggregate Scores: {'nDCG': [0.17308, 0.16929, 0.20292, 0.2415], 'Recall': [0.06346, 0.1591, 0.23716, 0.32639], 'collection': 'mt-rag-clapnq-elser-512-100-20240503', 'count': 208}\n",
      "Weighted average Recall: [0.05444060489060489, 0.13216625482625483, 0.18188963963963964, 0.2443027927927928]\n",
      "Weighted average nDCG: [0.12870239382239382, 0.1325306821106821, 0.15460135135135133, 0.18092346203346202]\n",
      "Done, scored file: outputs/bge15_questions_score.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_name = \"questions\"\n",
    "COLLECTIONS = {\n",
    "    \"clapnq\": {\n",
    "        \"collection_name\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "        \"root\": \"human/retrieval_tasks/clapnq\",\n",
    "        \"corpus_file\": \"clapnq.jsonl\",\n",
    "        \"query_file\": f\"clapnq_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"fiqa\": {\n",
    "        \"collection_name\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "        \"root\": \"human/retrieval_tasks/fiqa\",\n",
    "        \"corpus_file\": \"fiqa.jsonl\",\n",
    "        \"query_file\": f\"fiqa_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"govt\": {\n",
    "        \"collection_name\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "        \"root\": \"human/retrieval_tasks/govt\",\n",
    "        \"corpus_file\": \"govt.jsonl\",\n",
    "        \"query_file\": f\"govt_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "    \"cloud\": {\n",
    "        \"collection_name\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "        \"root\": \"human/retrieval_tasks/cloud\",\n",
    "        \"corpus_file\": \"cloud.jsonl\",\n",
    "        \"query_file\": f\"cloud_{task_name}.jsonl\",\n",
    "        \"qrels_file\": \"qrels/dev.tsv\",\n",
    "    },\n",
    "}\n",
    "model_name = \"bge15\"\n",
    "\n",
    "SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "sub_path = build_submission(SUB_PATH)\n",
    "run_official_eval(SUB_PATH, SCORED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bge15small\"\n",
    "# task_name = \"lastturn\"\n",
    "# print(model_name, task_name)\n",
    "# SUB_PATH = f\"outputs/{model_name}_{task_name}.jsonl\"\n",
    "# SCORED_PATH = f\"outputs/{model_name}_{task_name}_score.jsonl\"\n",
    "\n",
    "# run_official_eval(SUB_PATH, SCORED_PATH)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8807065,
     "sourceId": 13828662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8830910,
     "sourceId": 13861662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8830919,
     "sourceId": 13861674,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8830925,
     "sourceId": 13861680,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8832305,
     "sourceId": 13863469,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
